{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c3c5d1",
   "metadata": {},
   "source": [
    "<center><p style=\"font-size:30px;font-family:consolas\">SOUNDCLOUD - Exploratory Data Analysis</p></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea1a1e",
   "metadata": {},
   "source": [
    "We have to install:\n",
    "- **Selenium**: use for working with browse automatically\n",
    "- **Webdriver-manager**: use for install Chronium without installing Chronium before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e277f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665b077",
   "metadata": {},
   "source": [
    "# 1. Data collection: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddcfc0f",
   "metadata": {},
   "source": [
    "## 1.1. Initialize: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d75c8",
   "metadata": {},
   "source": [
    "Function `initialize_driver` use for initialize some Chrome options (ex: not load image, headless mode,...) and open specified url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4058834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize webdriver\n",
    "def initialize_driver(url):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs={\"profile.managed_default_content_settings.images\": 2, 'disk-cache-size': 4096 }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs) # Manage image loading and run on disk cache\n",
    "    chrome_options.add_argument(\"--headless\") # Runs Chrome in headless mode\n",
    "    chrome_options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage') # overcome limited resource problems\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "    driver.get(url)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ec3b2",
   "metadata": {},
   "source": [
    "`scrollWebpage` use for scrolling webside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a556de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollWebpage(n_times, driver):\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(n_times):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Wait to load page\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # compare 2 height (to check whether reach the end of website)\n",
    "        if new_height == last_height:\n",
    "            # webpage may be loading\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "        \n",
    "        # update height\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806dfaa",
   "metadata": {},
   "source": [
    "below function use for accept cookies if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7821bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_cookies(driver):\n",
    "    try:\n",
    "      WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH,\n",
    "                                                        '//button[@id=\"onetrust-accept-btn-handler\"]')))\n",
    "    except Exception as err:\n",
    "      print(\"there's no cookies\")\n",
    "    else:\n",
    "      ck = driver.find_element_by_xpath('//button[@id=\"onetrust-accept-btn-handler\"]') # accept cookies\n",
    "      driver.execute_script(\"arguments[0].click();\", ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11caf4",
   "metadata": {},
   "source": [
    "convert string to number (because almost crawled data from web are string type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2number(numstr):\n",
    "    if numstr == \"Repost\" or numstr == \"Like\":\n",
    "        return 0\n",
    "\n",
    "    numstr = numstr.replace(',', '')\n",
    "    mul = 1\n",
    "    if numstr[-1] == 'K':\n",
    "        numstr = numstr.replace('K', '')\n",
    "    elif numstr[-1] == 'M':\n",
    "        numstr = numstr.replace('M', '')\n",
    "        mul = 1000000\n",
    "    elif numstr[-1] == 'B':\n",
    "        numstr = numstr.replace('B', '')\n",
    "        mul = 1000000000\n",
    "    return int(float(numstr) * mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15534e5",
   "metadata": {},
   "source": [
    "Function `write_csv_file` use for writing a list data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72cd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_file(filename, header, content):\n",
    "    with open(filename, 'w', encoding='utf-8-sig', newline='') as file:\n",
    "        writter = csv.writer(file, delimiter=',')\n",
    "        writter.writerow(header)\n",
    "        for l in content:\n",
    "            writter.writerow(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30378125",
   "metadata": {},
   "source": [
    "Initialize some common variable for some task (assign, storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e402c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some ultilities\n",
    "sleep = 3\n",
    "\n",
    "searchStr = 'abcdefghijklmnoprstwuyxyz123456789'\n",
    "base_link = 'https://www.soundcloud.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736e76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variable for assigning and storing\n",
    "users_id = dict() # url : id\n",
    "users_list = [] # all data\n",
    "users_id_assign = 0 # use for assigning id\n",
    "unknow_users = dict() # url : [id, username] --> use temporary store un-stored users\n",
    "\n",
    "playlists_id = dict()\n",
    "playlists_list = []\n",
    "playlists_id_assign = 0\n",
    "\n",
    "tracks_id = dict() # url : [id, trackname]\n",
    "tracks_list = []\n",
    "tracks_id_assign = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee0259",
   "metadata": {},
   "source": [
    "## 1.2. Crawling users (initialize some functions to crawl user data): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1ab867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlUsersData_from_search(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    users = soup.find_all('div', class_=\"userItem sc-media g-flex-row-centered sc-px-2x sc-py-1x m-horizontal m-verified\")\n",
    "    \n",
    "    global users_id_assign\n",
    "    for user in users:\n",
    "        user_url = base_link + user.find('a', class_=\"userItem__coverArt sc-media-image sc-mr-2x\")['href']\n",
    "        if user_url in users_id.keys():\n",
    "            continue\n",
    "        \n",
    "        username = user.find('a', class_=\"sc-link-dark sc-link-primary\").text.strip().encode('utf-8').decode('utf-8')\n",
    "        \n",
    "        confirm_owner = user.find('span', class_=\"sc-status-icon sc-status-icon-verified sc-status-icon-small sc-ir\")\n",
    "        if confirm_owner:\n",
    "            confirm_owner = 'Yes'\n",
    "        else:\n",
    "            confirm_owner = 'Not yet'\n",
    "        \n",
    "        user_followers = user.find('li', class_=\"sc-ministats-item\")\n",
    "        if user_followers:\n",
    "            user_followers = int(user.find('li', class_=\"sc-ministats-item\")['title'].split(' ')[0].replace(',', ''))\n",
    "        else:\n",
    "            user_followers = 0\n",
    "        \n",
    "        user_detail = user.find('h3', \\\n",
    "                                class_='userItem__details sc-type-light sc-text-secondary sc-text-h4 sc-mt-0.5x')\\\n",
    "                                .find_all('div')\n",
    "        detail_name, country = tuple(map(lambda x : x.text.strip().encode('utf-8').decode('utf-8'), user_detail))\n",
    "        \n",
    "        user_id = users_id_assign\n",
    "        users_id_assign += 1\n",
    "        users_id[user_url] = user_id\n",
    "        \n",
    "        users_list.append([user_id, username, user_url, confirm_owner, detail_name, country, user_followers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e454dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlUsersData_from_page(url):\n",
    "    driver = initialize_driver(url)\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH,\n",
    "                                                            '//button[@id=\"onetrust-accept-btn-handler\"]')))\n",
    "    ck = driver.find_element_by_xpath('//button[@id=\"onetrust-accept-btn-handler\"]')\n",
    "    driver.execute_script(\"arguments[0].click();\", ck)\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    username = unknow_users[url][0]\n",
    "    \n",
    "    if soup.find('span', class_=\"verifiedBadge userDropbar__verifiedBadge\"):\n",
    "        confirm_owner = 'Yes'\n",
    "    else:\n",
    "        confirm_owner = 'Not yet'\n",
    "        \n",
    "    user_followers = int(soup.find('a', class_=\"infoStats__statLink sc-link-light sc-link-primary\")['title']\\\n",
    "                         .split(' ')[0].replace(',', ''))\n",
    "\n",
    "    detail_name = soup.find('h3', \n",
    "                            class_=\"profileHeaderInfo__additional g-type-shrinkwrap-block theme-dark g-type-shrinkwrap-large-secondary\")\n",
    "    if detail_name:\n",
    "        detail_name = detail_name.text.strip().encode('utf-8').decode('utf-8')\n",
    "    else:\n",
    "        detail_name = ''\n",
    "        \n",
    "    country = soup.find('h3', \n",
    "                        class_='profileHeaderInfo__additional g-type-shrinkwrap-block theme-dark g-type-shrinkwrap-large-secondary sc-mt-1x')\n",
    "    if country:\n",
    "        country = country.text.strip().encode('utf-8').decode('utf-8')\n",
    "    else:\n",
    "        country = ''\n",
    "\n",
    "    user_id = unknow_users[url][1]\n",
    "\n",
    "    users_list.append([user_id, username, url, confirm_owner, detail_name, country, user_followers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f5eef",
   "metadata": {},
   "source": [
    "## 1.3. Crawling playlists (initialize some functions to crawl playlist data) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a347043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funciton is temporarily unavailable\n",
    "def CrawlPlaylistData_from_page(url):\n",
    "    global playlists_id_assign\n",
    "    global users_id_assign\n",
    "    global tracks_id_assign\n",
    "    \n",
    "    # open driver\n",
    "    driver = initialize_driver(url)\n",
    "    accept_cookies(driver)\n",
    "    scrollWebpage(1000, driver)\n",
    "    page_source = driver.page_source\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # crawl\n",
    "    pll_name = soup.find('h1', \n",
    "                         class_='soundTitle__title sc-font g-type-shrinkwrap-inline g-type-shrinkwrap-large-primary theme-dark')\\\n",
    "                        .text.strip().encode('utf-8').decode('utf-8')\n",
    "    #crawl user information\n",
    "    pll_user = soup.find('a', \n",
    "                         class_='userBadge__usernameLink sc-link-dark sc-link-primary sc-truncate sc-mr-0.5x')\n",
    "    pll_username = pll_user.text.strip().encode('utf-8').decode('utf-8')\n",
    "    pll_user_url = pll_user['href']\n",
    "    if pll_user_url not in users_id.keys() and pll_user_url not in unknow_users.keys():\n",
    "        unknow_users['pll_url'] = [users_id_assign, pll_username]\n",
    "        users_id_assign += 1\n",
    "        \n",
    "    pll_time = soup.find('time', class_='relativeTime')['datetime']\n",
    "    \n",
    "    pll_tags = soup.find_all('span', class_='sc-truncate sc-tagContent')\n",
    "    if pll_tags:\n",
    "        pll_mainTag = pll_tags[0].text.strip().encode('utf-8').decode('utf-8')\n",
    "        pll_detailTags = list(map(lambda x : x.text.strip().encode('utf-8').decode('utf-8'), pll_tags[1:]))\n",
    "    else:\n",
    "        pll_mainTag = ''\n",
    "        pll_detailTags = []\n",
    "    \n",
    "    pll_stat = soup.find_all('li', class_='sc-ministats-item')\n",
    "    pll_like = int(pll_stat[0]['title'].strip().split(' ')[0].replace(',', ''))\n",
    "    pll_repost = int(pll_stat[1]['title'].strip().split(' ')[0].replace(',', ''))\n",
    "    \n",
    "    # crawl tracks\n",
    "    tracks = soup.find_all('div', class_='trackItem__content sc-truncate')\n",
    "    pll_tracks_id = []\n",
    "    for track in tracks:\n",
    "        track_url = base_link + track.find('a',\n",
    "                                          class_='trackItem__trackTitle sc-link-dark sc-link-primary sc-font-light')['href']\\\n",
    "                                            .split('?')[0]\n",
    "        if track_url not in tracks_id.keys():\n",
    "            track_id = tracks_id_assign\n",
    "            tracks_id_assign += 1\n",
    "            tracks_id[track_url] = track_id\n",
    "        else:\n",
    "            track_id = tracks_id[track_url]\n",
    "        \n",
    "        pll_tracks_id.append(track_id)\n",
    "        \n",
    "        pll_url = str(url)\n",
    "    playlists_list.append([pll_name, pll_url, playlists_id[url], pll_username, pll_time, pll_mainTag, \n",
    "                     pll_detailTags, pll_like, pll_repost, len(pll_tracks_id), pll_tracks_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba1530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlPlaylistData_from_search(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    global playlists_id_assign\n",
    "    global tracks_id_assign\n",
    "    global users_id_assign\n",
    "    # print(len(playlists))\n",
    "\n",
    "    playlists = soup.find_all('div', class_='sound__body')\n",
    "    for playlist in playlists:\n",
    "        playlist_base = playlist.find('a', class_='sc-link-primary soundTitle__title sc-link-dark sc-text-h4')\n",
    "        playlist_url = base_link + playlist_base['href']\n",
    "        if playlist_url in playlists_id.keys():\n",
    "            continue\n",
    "        \n",
    "        playlist_available = playlist.find('span', class_='g-geoblocked-icon')\n",
    "        if playlist_available:\n",
    "            continue\n",
    "        \n",
    "        playlist_id = playlists_id_assign\n",
    "        playlists_id[playlist_url] = playlist_id\n",
    "        playlists_id_assign += 1\n",
    "        \n",
    "        playlist_name = playlist_base.text.strip().encode('utf-8').decode('utf-8')\n",
    "\n",
    "        playlist_time = playlist.find('time', class_=\"relativeTime sc-text-secondary sc-text-captions\")['datetime']\n",
    "\n",
    "        playlist_tag = playlist.find('span', class_='sc-truncate sc-tagContent')\n",
    "        if playlist_tag:\n",
    "            playlist_tag = playlist_tag.text.strip().encode('utf-8').decode('utf-8')\n",
    "        else:\n",
    "            playlist_tag = ''\n",
    "\n",
    "        playlist_like = string2number(playlist.find('button', class_=\"sc-button-like sc-button-secondary sc-button sc-button-small sc-button-responsive\").text)\n",
    "\n",
    "        playlist_repost = string2number(playlist.find('button', class_=\"sc-button-repost sc-button-secondary sc-button sc-button-small sc-button-responsive\").text)\n",
    "\n",
    "        # crawl user\n",
    "        playlist_username = playlist.find('span', class_='soundTitle__usernameText').text.strip().encode('utf-8').decode('utf-8')\n",
    "        playlist_user_url = base_link + playlist.find('span', class_='soundTitle__usernameText').parent['href']\n",
    "\n",
    "        if playlist_user_url not in users_id:\n",
    "            user_id = users_id_assign\n",
    "            users_id[playlist_user_url] = user_id\n",
    "            users_id_assign += 1\n",
    "\n",
    "            users_list.append([user_id, playlist_username, playlist_user_url, '', '', '', ''])\n",
    "        else:\n",
    "            user_id = users_id[playlist_user_url]\n",
    "\n",
    "        # crawl tracks\n",
    "        playlist_tracks = playlist.find_all('div', class_='compactTrackListItem sc-media sc-border-light-bottom clickToPlay m-interactive m-playable')\n",
    "        playlist_tracks_id = []\n",
    "        for track in playlist_tracks:\n",
    "            # this is new version\n",
    "            track_url = base_link + track.find('span', class_='compactTrackListItem__trackTitle sc-text-primary sc-text-h4')['data-permalink-path'].split('?')[0]\n",
    "            if track_url not in tracks_id:\n",
    "                track_id = tracks_id_assign\n",
    "                tracks_id[track_url] = track_id\n",
    "                tracks_id_assign += 1\n",
    "            else:\n",
    "                track_id = tracks_id[track_url]\n",
    "                \n",
    "            playlist_tracks_id.append(track_id)\n",
    "\n",
    "        playlists_list.append([playlist_id, playlist_name, playlist_url, user_id, playlist_username, playlist_time, playlist_tag, \n",
    "                                playlist_like, playlist_repost, len(playlist_tracks_id), playlist_tracks_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4512255",
   "metadata": {},
   "source": [
    "## 1.4. Crawling tracks (initialize function to crawl tracks data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecd92080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlingTracksData_from_page(url):\n",
    "    driver = initialize_driver(url)\n",
    "    accept_cookies(driver)\n",
    "    scrollWebpage(10, driver)\n",
    "    page_source = driver.page_source\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    track_name = soup.find('h1', class_=\"soundTitle__title sc-font g-type-shrinkwrap-inline g-type-shrinkwrap-large-primary theme-dark\")\\\n",
    "                    .text.strip().encode('utf-8').decode('utf-8')\n",
    "    track_username = soup.find('a', class_=\"sc-link-secondary\").text.strip().encode('utf-8').decode('utf-8')\n",
    "    \n",
    "    track_time = soup.find('time', class_='relativeTime')['datetime']\n",
    "\n",
    "    track_tag = soup.find('a', class_=\"sc-tag sc-tag-large\")\n",
    "    if track_tag:\n",
    "        track_tag = track_tag.text.strip().encode('utf-8').decode('utf-8')\n",
    "    else:\n",
    "        track_tag = ''\n",
    "    \n",
    "    track_stat = soup.find('ul', class_=\"soundStats sc-ministats-group listenEngagement__stats sc-ministats-group-right\")\n",
    "    track_play, track_like, track_repost = tuple(map(lambda x : int(x['title'].strip().split(' ')[0].replace(',', '')),\n",
    "                                                    track_stat.find_all('li', class_=\"sc-ministats-item\")))\n",
    "\n",
    "    tracks_list.append([tracks_id[url], track_name, url, track_time, track_tag, track_play, track_like, track_repost])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e7fef",
   "metadata": {},
   "source": [
    "## 1.5. Crawling (start crawling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64d14fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_search_str = [base_link + '/search/people?q=' + c for c in searchStr]\n",
    "# page_sources = []\n",
    "def get_users(url):\n",
    "    driver = initialize_driver(url)\n",
    "    accept_cookies(driver)\n",
    "    scrollWebpage(5, driver)\n",
    "    page_source = driver.page_source\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    \n",
    "    CrawlUsersData_from_search(page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e497c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4892\\2889666167.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4892\\3544239256.py:8: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  ck = driver.find_element_by_xpath('//button[@id=\"onetrust-accept-btn-handler\"]') # accept cookies\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    executor.map(get_users, people_search_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff80be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_search_str = [base_link + '/search/sets?q=' + c for c in searchStr]\n",
    "\n",
    "def get_playlists(url):\n",
    "    driver = initialize_driver(url)\n",
    "    accept_cookies(driver)\n",
    "    scrollWebpage(7, driver)\n",
    "\n",
    "    try:\n",
    "        more_tracks_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//a[@class=\"compactTrackList__moreLink sc-link-light sc-link-primary sc-border-light sc-text-h4\"]')))\n",
    "    except Exception as err:\n",
    "        print('Something went wrong!')\n",
    "    else:\n",
    "        for bt in more_tracks_button:\n",
    "            driver.execute_script(\"arguments[0].click();\", bt)\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "\n",
    "    CrawlPlaylistData_from_search(page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "160c0228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4892\\2889666167.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4892\\3544239256.py:8: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  ck = driver.find_element_by_xpath('//button[@id=\"onetrust-accept-btn-handler\"]') # accept cookies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    executor.map(get_playlists, playlists_search_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f4496b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tracks_url = random.choices(list(tracks_id.keys()), k=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c82303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4892\\2889666167.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there's no cookies\n",
      "there's no cookiesthere's no cookies\n",
      "\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4892\\3544239256.py:8: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  ck = driver.find_element_by_xpath('//button[@id=\"onetrust-accept-btn-handler\"]') # accept cookies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n",
      "there's no cookies\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    executor.map(CrawlingTracksData_from_page, crawl_tracks_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b69c2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_header = ['user_id', 'username', 'user_url', 'confirm_owner', 'detail_name', 'country', 'user_followers']\n",
    "write_csv_file('./user.csv', user_header, users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c10e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_header = ['playlist_id', 'playlist_name', 'playlist_url', 'owner_id', 'username', 'playlist_time', 'playlist_tag', \n",
    "                                'like', 'repost', 'size', 'tracks']\n",
    "write_csv_file('./playlist.csv', playlist_header, playlists_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aac182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_header=['track_id', 'track_name', 'url', 'datetime', 'track_tag', 'track_play', 'track_like', 'track_repost']\n",
    "write_csv_file('./track.csv', track_header, tracks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
